{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition: CNN for Spoken Language\n",
    "畳み込みニューラルネットワークによる音声認識  \n",
    "- https://chsasank.github.io/spoken-language-understanding.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SR.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要点: 波形の形状のパターンを学習することで、畳み込みによって特徴を抽出する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 手順\n",
    "- 音声データ（Dataset: ）\n",
    "- 波形獲得（.wav）\n",
    "- 特徴抽出（転移学習: ）\n",
    "- モデル構築（CNN）\n",
    "\n",
    "### Terminology 専門用語\n",
    "- メル周波数ケプストラム係数（MFCC）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds = pd.read_csv('UrbanSound8K.csv')\n",
    "sounds.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一定の長さ以上のデータのみを対象にする 3秒以上\n",
    "sounds = sounds[sounds[\"start\"] - sounds[\"end\"] >= 3]\n",
    "sounds.set_index(\"fsID\")\n",
    "sounds = sounds[[\"slice_file_name\", \"classID\", \"fold\"]]\n",
    "sounds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path1 = \"/Users/akr712/Desktop/音声認識/UrbanSound8K/audio/fold6/135160-8-0-0.wav\"\n",
    "# 音声データの読み込み\n",
    "y, sr = librosa.load(path1, duration=3.0)\n",
    "# メル周波係数の取得\n",
    "ps = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akr712/Desktop/音声認識'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.display.specshow(ps, y_axis=\"mel\", x_axis=\"time\", cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chirdlen\n",
    "path2 = \"/Users/akr712/Desktop/音声認識/UrbanSound8K/audio/fold6/135160-8-0-0.wav\"\n",
    "y, sr = librosa.load(path2, duration=3.0)\n",
    "ps2 = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "librosa.display.specshow(ps2, y_axis=\"mel\", x_axis=\"time\", cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chirdlen\n",
    "path2 = \"/Users/akr712/Desktop/音声認識/UrbanSound8K/audio/fold6/135160-8-0-0.wav\"\n",
    "y, sr = librosa.load(path2, duration=3.0)\n",
    "ps2 = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "librosa.display.specshow(ps2, y_axis=\"mel\", x_axis=\"time\", cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"\", ]\n",
    "base_path = \"/Users/akr712/Desktop/音声認識/UrbanSound8K/audio/\"\n",
    "sounds[\"path\"] = base_path + sounds['fold'].astype(\"str\") + \"/\" +sounds[\"slice_file_name\"].astype(\"str\")\n",
    "sounds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://dkopczyk.quantee.co.uk/wp-content/uploads/2018/08/wav-768x132.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akr712/Desktop/音声認識'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listにデータを集める\n",
    "speech_datas = {}\n",
    "for row in sounds.itertuples():\n",
    "    y, sr = librosa.load(row[3], duration=2.97)  \n",
    "    ps = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    if ps.shape != (128, 128): \n",
    "        continue\n",
    "    speech_datas[row[1]] = ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスをバラバラにするために一応シャッフル\n",
    "from random import shuffle\n",
    "\n",
    "melspectrograms = list(speech_datas.values())\n",
    "shuffle(melspectrograms)\n",
    "speech_datas = dict(zip(speech_datas.keys(), melspectrograms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Image Data into 3D for 2D CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape and データ作成\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "x_test = []\n",
    "counter = 0\n",
    "\n",
    "for label, ps in speech_datas.items():\n",
    "    \n",
    "    label = np.array(to_categorical(label, 10))\n",
    "    ps = np.array(ps.reshape((128, 128, 1)))\n",
    "    \n",
    "    if counter < 7000:\n",
    "        x_train.append(ps)\n",
    "        y_train.append(label)\n",
    "        counter += 1\n",
    "    else:\n",
    "        x_test.append(ps)\n",
    "        y_test.append(label)\n",
    "        counter += 1\n",
    "        \n",
    "counter\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Build CNN Model\n",
    "### CNN: Convolutional Neural Networks\n",
    "The basic architecture of CNN includes:\n",
    "1. Convolutional Layer – uses convolutional operator to filter input signal and extract some additional image features\n",
    "2. Activation Function – applies non-linear function such as rectifier to the outputs of convolutional layer\n",
    "3. Pooling Layer – performs a downsampling operation reducing the size of an input with max() or sum() operation\n",
    "4. Fully-Connected Layer – each neuron in the previous layer is connected to each neuron on the next layer with last such layer producing outputs of neural network.\n",
    "\n",
    "![](https://bookdown.org/wshuyi/dive-into-data-science-practically/assets/2018-06-27-11-12-04-076004.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization,Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "features_shape = (128, 128, 1)\n",
    "\n",
    "inputs = Input(shape=features_shape)\n",
    "\n",
    "# Block 1\n",
    "o = Conv2D(24, (5, 5), strides=(1, 1), input_shape=features_shape)(inputs)\n",
    "o = MaxPooling2D(pool_size=(4, 2), strides=(4, 2))(o)\n",
    "# o = Activation('relu')(o)\n",
    "o = BatchNormalization()(o)\n",
    "\n",
    "# Block 2\n",
    "o = Conv2D(48, (5, 5), padding=\"valid\")(o)\n",
    "o = MaxPooling2D((4, 2), strides=(4, 2))(o)\n",
    "# o = Activation('relu')(o)\n",
    "o = BatchNormalization()(o)\n",
    "\n",
    "# Block 3\n",
    "o = Conv2D(48, (5, 5), padding=\"valid\")(o)\n",
    "o = Activation(\"relu\")(o)\n",
    "\n",
    "# Flatten\n",
    "o = Flatten()(o)\n",
    "# o = Dropout(rate=0.5)(o)\n",
    "\n",
    "# Dense layer\n",
    "o = Dense(64, activation=\"relu\")(o)\n",
    "o = BatchNormalization()(o)\n",
    "o = Dropout(rate=0.5)(o)\n",
    "\n",
    "# Predictions\n",
    "outputs = Dense(10, activation=\"softmax\")(o)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Training CNN for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=12, batch_size=128,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = history.history['acc']\n",
    "test_acc = history.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_acc) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_acc, 'r--')\n",
    "plt.plot(epoch_count, test_acc, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今後: Data Augumentation を使った場合に分類の精度が向上するか試したい"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
